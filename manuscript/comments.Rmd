---
title: Post-publication notes on the deconvolution paper 
author: Aaron Lun
output:
  BiocStyle::html_document:
    fig_caption: false
    toc_float: true
---

# Dealing with zero counts

## Origin of zeroes 

Semi-systematic zeros are probably more relevant in single-cell RNA-seq data than in bulk.
In bulk, you'd expect some contamination so the count would unlikely to be exactly zero in any particular sample.
By comparison, each cell (aside from doublets) is of a single type.
When genes are silenced in that cell type, you can get zero counts occurring in a subset of your samples. 

## Dealing with dropouts 

Differences in dropout rates between cells should not affect scaling normalization.
This is because scaling normalization concerns itself with systematic (technical) differences in the **expected** expression between cells.
Whether such differences occur due to changes in the zero or non-zero components is irrelevant to the computed size factors. 
The distribution of expression only matters insofar as the robust average estimators are accurate.

# Inaccuracies with existing methods

## Applying existing methods on the highest abundances

The alternative approaches would be to use the arithmetic mean (for DESeq) or to remove a gene entirely if it contains any zeroes.
The former still results in biases, as a majority of zeroes in each cell means that cell-specific zeroes must be removed.
The latter leaves us with a set of high-abundance genes from which the size factor is computed.
This makes the non-DE assumption stronger; everything else being equal, it's easier to get a majority in a small set than a larger one.

One could argue that the high-abundance genes are more likely to be house-keeping genes.
However, that doesn't make them non-DE, especially when you throw in biological variability and coregulation driving systematic differences between cells.
Finally, it's not a sustainable strategy, as the set of non-zero genes will decrease with an increasing number of cells.
This will leave you with very few genes -- possibly in the double-digits -- which exacerbates the problem above.

Indeed, completely removing genes with any zero can directly bias the size factor estimates.
This is because you are more likely to select genes that are upregulated in cells with small size factors, in order to avoid zeroes.
Such upregulation can be genuine DE, or stochastic due to non-independent filtering.
This results in overestimation of the size factors for such cells:

```{r}
set.seed(10000)
means <- cbind(matrix(1, nrow=1000, ncol=20), matrix(2, nrow=1000, ncol=20))
means[1:50, 1:20] <- 10 # Upregulated gene
counts <- matrix(rnbinom(length(means), mu=means, size=20), nrow=nrow(means), ncol=ncol(means))
nonzero <- rowSums(counts==0)==0
sf <- DESeq2::estimateSizeFactorsForMatrix(counts[nonzero,])
plot(means[51,], sf)
```

Precision also drops with this strategy, which becomes relevant for the median-based estimator when you're dealing with few genes.
Pooling avoids the decrease in precision by allowing more genes to be used when computing the median.

```{r}
means <- matrix(5, nrow=1000, ncol=50)
counts <- matrix(rnbinom(length(means), mu=means, size=20), nrow=nrow(means), ncol=ncol(means))
for (x in c(10, 20, 50, 100, 200, 500, 1000)) { 
    nonzero <- rowSums(counts[1:x,]==0)==0
    sf <- DESeq2::estimateSizeFactorsForMatrix(counts[1:x,][nonzero,])
    print(mad(log(sf/1)))
}
```

## Explaining the zero-induced biases in existing methods

The explanation in the main text does not need to consider undefined ratios (i.e., due to a zero in the denominator).
For DESeq, there are none, as all ratios are defined after the workaround.
For edgeR, the number of undefined ratios that are removed is constant when the same reference cell is used for each comparison.
Thus, differences will be driven by zeroes in the numerator of the ratios.

# Supplementary details

## Performance of DESeq normalization after addition of a pseudo-count

The flipped away-from-unity bias with library size-adjusted pseudo counts is an interesting effect.
This is probably caused by the lack of precision of the median-based estimator, especially when it is occurring at a count that was originally zero.
For simplicity, assume that the library size is the true size factor, so that the ratio of each gene's mean count to the average reference would be a perfect estimate.
(This is true even with addition of a pseudo count, due to the library size adjustment.)
However, the median ratio for each cell will be lower than the expected ratio if it is computed from a count that was originally zero.
This results in underestimation of the size factor for small cells, and concomitant overestimation for the large cells after centering.
We get a smooth line rather than a sharp jump from zero to 1, as the ratio will steadily increase due to the prior count. 

Obviously, this is a moot point as it depends on the library size being an accurate estimate of the size factor.
This won't be true with DE, which is the entire point of using alternative normalization procedures in the first place.

## Comparing normalization accuracy on real data

The low MAD for DESeq is probably because of its inaccuracy, where the size factors are constrained by discreteness.
At higher counts, DESeq and deconvolution approach the same precision, which makes sense as they are computed in basically the same manner.
Library size normalization is most efficient at using information but obviously is only applicable when there is no DE.
We can increase precision for deconvolution by using more fine-grained sizes (set sizes=2:10*10 in standerr.R), but this comes at the cost of computational work.
